{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 0 ns, sys: 0 ns, total: 0 ns\n",
      "Wall time: 7.15 Âµs\n",
      "The autotime extension is already loaded. To reload it, use:\n",
      "  %reload_ext autotime\n",
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "time: 43.6 ms\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "%load_ext autotime\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import os\n",
    "import sys\n",
    "parent_dir = os.path.abspath(os.getcwd()+'/..')+'/' \n",
    "sys.path.append(parent_dir) \n",
    "\n",
    "from utils.path import dir_HugeFiles\n",
    "from utils.words import make_corpus_0, clean_wordcount, get_wordcount, replace_UNK, parse_section\n",
    "from utils.preprocessing import load, preprocessing, clean_ny\n",
    "from utils.save import make_dir, save_pickle, load_pickle, auto_save_csv, print_time\n",
    "###\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#dir_save = os.path.normpath(dir_HugeFiles+'preprocessing/dic_20190504.pickle')\n",
    "#dic = load(dir_save)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exist\n",
      "time: 3.72 s\n"
     ]
    }
   ],
   "source": [
    "dirname = '/data/yueliu/RecipeAnalytics_201902/All_Recipe_01_03_2019/json/'\n",
    "dir_save = dir_HugeFiles+'dph/allrecipes.pickle'\n",
    "#data = preprocessing(dir_save, dirname)\n",
    "data = load(dir_save)\n",
    "data = data.rename(index = str, columns = {'sections':'_sections'})\n",
    "proc_data = data[[col for col in data.columns if col[0] == '_' and col not in ['_time','_followers_count']]]\n",
    "proc_data.columns = [col[1:] for col in proc_data.columns] # rename the columns\n",
    "proc_data = proc_data.dropna().reset_index(drop=True)\n",
    "dry_weight_g = ['protein','sugars', 'total carbohydrates', 'total fat']\n",
    "dry_weight_mg = [col for col in proc_data.select_dtypes([float]) if col not in dry_weight_g+['servings','calorie']]\n",
    "proc_data['dry_weight'] = proc_data[dry_weight_g].sum(axis = 1)+0.001*proc_data[dry_weight_mg].sum(axis = 1)\n",
    "for col in proc_data.select_dtypes([float]):\n",
    "    if col not in ['dry_weight','servings']:\n",
    "        if col in dry_weight_g+['calorie']:\n",
    "            proc_data[col+'/dry_weight'] = proc_data[col]/proc_data['dry_weight']\n",
    "        else:\n",
    "            #from mg to g\n",
    "            proc_data[col+'/dry_weight'] = 0.001*proc_data[col]/proc_data['dry_weight']\n",
    "proc_data['sugars/dietary fiber'] = proc_data['sugars']/proc_data['dietary fiber']\n",
    "proc_data['dietary fiber adjusted'] = [0.00001 if ele == 0.0 else ele for ele in proc_data['dietary fiber']]\n",
    "proc_data['sugars/dietary fiber adjusted'] = proc_data['sugars']/proc_data['dietary fiber adjusted']\n",
    "proc_data['GI'] = [1 if 'low glycemic impact recipes'in i else 0 for i in proc_data['tags']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 8.27 s\n"
     ]
    }
   ],
   "source": [
    "dic = proc_data.to_dict(orient='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1.07 s\n"
     ]
    }
   ],
   "source": [
    "for i, v in dic.items():\n",
    "    dry_weight = v['protein']+v['sugars']+v['total carbohydrates']+v['total fat']\n",
    "    dry_weight += 0.001*(v['cholesterol']+v['calcium']+v['sodium']+v['potassium']+v['vitamin c']+\n",
    "                     v['iron']+v['thiamin']+v['niacin']+v['vitamin b6']+v['magnesium']+v['folate'])\n",
    "    dic[i]['dry_weight'] = dry_weight\n",
    "    \n",
    "    for nu in ['cholesterol','calcium','sodium','potassium',\n",
    "               'vitamin c','iron','thiamin','niacin','vitamin b6','magnesium','folate','vitamin a']:\n",
    "        dic[i][nu+'/dry_weight'] = v[nu]/dic[i]['dry_weight']*0.001\n",
    "    for nu in ['protein','sugars','total carbohydrates','total fat', 'dietary fiber','saturated fat','calorie']:\n",
    "        dic[i][nu+'/dry_weight'] = v[nu]/dic[i]['dry_weight']\n",
    "    if 'low glycemic impact recipes' in dic[i]['tags']:\n",
    "        dic[i]['GI'] = 1\n",
    "    else:\n",
    "        dic[i]['GI'] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### use NYtimes-parser to process the ingredients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1.87 s\n"
     ]
    }
   ],
   "source": [
    "dir_save2 = os.path.normpath(dir_HugeFiles+'preprocessing/ingred_20190607.txt')\n",
    "overwrite = True\n",
    "if overwrite:\n",
    "    with open(dir_save2,'w') as f:\n",
    "        list_idid = []\n",
    "        for i, value in dic.items():\n",
    "            for j, ingred in enumerate(value['ingredients']):\n",
    "                #assert type(ingred[0]) == str\n",
    "                #assert ingred != \"\"\n",
    "                if not ingred == '' and ' ':\n",
    "                    pair = [i, j, ingred]\n",
    "                    list_idid.append(pair)\n",
    "                    f.write(\"%s\\n\" % ingred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "use python2 <br>\n",
    "cd /home/helena/NYtime-parser2/ <br>\n",
    "python bin/parse-ingredients.py /data/yueliu/RecipeAnalytics_201902/preprocessing/ingred_20190302.txt > results_20190302.txt <br>\n",
    "python bin/convert-to-json.py results_20190302.txt > results_20190302.json <br>\n",
    "move the results_20190302.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 16.3 s\n"
     ]
    }
   ],
   "source": [
    "dir_save2 = os.path.normpath('/data/yueliu/RecipeAnalytics_201902/preprocessing/results_20190302.json')\n",
    "df_idid = pd.DataFrame(list_idid, columns = ['index_1','index_2','ingred'])\n",
    "parsed = pd.read_json(dir_save2)\n",
    "df_idid[['input','qty','unit','name','other']] =  parsed[['input','qty','unit','name','other']]\n",
    "df_parsed = df_idid.groupby('index_1')[['name']].agg(lambda x: list(x)).reset_index(drop = True)\n",
    "for i, value in dic.items():\n",
    "    dic[i]['ingred_ny'] = clean_ny(df_parsed.iloc[i]['name'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### drop too short sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "drop 46 recipes with less than 2 ingredients\n",
      "time: 54.3 ms\n"
     ]
    }
   ],
   "source": [
    "ls = [i for i,v in dic.items() if len(v['ingredients'])>1]\n",
    "print('drop %d recipes with less than 2 ingredients' %(len(dic)-len(ls)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "furthur drop 1026 recipes with less than 2 instructions\n",
      "time: 56.5 ms\n"
     ]
    }
   ],
   "source": [
    "ls = [i for i in ls if len(dic[i]['directions'])>1]\n",
    "print('furthur drop %d recipes with less than 2 instructions' %(len(dic)-len(ls)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### try to get the words with occurrences > 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 38.8 s\n"
     ]
    }
   ],
   "source": [
    "# still can distinguish name and ingredient... : corpus_llist\n",
    "# treat both name and ingredients as sentences :corpus_list\n",
    "corpus_llist, corpus_list, corpus = make_corpus_0(dic, ['name','ingredients','directions'], ls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### replace rare words by UNK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 23.7 ms\n"
     ]
    }
   ],
   "source": [
    "def replace_UNK(recipe, knowns, space):\n",
    "    if type(recipe[0]) == list:\n",
    "        return [replace_UNK(sent, knowns, space) for sent in recipe]\n",
    "    else:\n",
    "        ans = [word if word in knowns else 'UNK' for word in recipe]\n",
    "        if space:\n",
    "            return ' '.join(ans)\n",
    "        else:\n",
    "            return ans "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 7min 3s\n"
     ]
    }
   ],
   "source": [
    "# take ~ 2 min\n",
    "keys = ['name','ingredients','directions']\n",
    "X, knowns = get_wordcount(corpus_list)\n",
    "if not knowns:\n",
    "    print('warn')\n",
    "for i, v in dic.items():\n",
    "    for k in keys:\n",
    "        v[k+'_list'] = clean_wordcount(v[k])\n",
    "        v[k+'_UNK'] = replace_UNK(v[k+'_list'], knowns, space = True)\n",
    "        v[k+'_UNK2'] = replace_UNK(v[k+'_list'], knowns, space = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### append ingredients_UNK2_none becuase prof.LIM think UNK feature is weird"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 5.25 s\n"
     ]
    }
   ],
   "source": [
    "words = ['UNK',\"(\",\"[\",\".\",\",\",\"!\",\"?\",\"(\",\")\"]\n",
    "def remove_UNK(line):\n",
    "    return [word for word in line if word not in words]\n",
    "\n",
    "keys = ['name','ingredients','directions']\n",
    "\n",
    "for i, v in dic.items():\n",
    "    for k in keys:\n",
    "        dic[i][k+'_UNK2_none'] = [remove_UNK(line) for line in v[k+'_UNK2']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1.33 s\n"
     ]
    }
   ],
   "source": [
    "from utils.words import clean_list, parse_section\n",
    "for i, v in dic.items():\n",
    "    v['sec'] = parse_section(v['sections'], v['categories'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 8.25 s\n"
     ]
    }
   ],
   "source": [
    "save_pickle(filename='../data/dic_20190819.pickle', obj=dic, overwrite=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
