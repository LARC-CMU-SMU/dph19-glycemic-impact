{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 0 ns, sys: 0 ns, total: 0 ns\n",
      "Wall time: 6.44 Âµs\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "%load_ext autotime\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import sys\n",
    "parent_dir = os.path.abspath(os.getcwd()+'/..')+'/' \n",
    "sys.path.append(parent_dir) \n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "\n",
    "from utils.path import dir_HugeFiles\n",
    "from utils.preprocessing import load\n",
    "from utils.save import make_dir, save_pickle, load_pickle, auto_save_csv, print_time, auto_save_pickle\n",
    "\n",
    "from models.fmin2 import fmin2, p2_lr, p2_lgbm\n",
    "from models.nested_validation import *\n",
    "from models.features import fixed_makedata, salvador_wrap, pretrained_wrap\n",
    "from models.display import pickle2df\n",
    "\n",
    "# models\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### loading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 216 ms\n"
     ]
    }
   ],
   "source": [
    "dic = load_pickle('../data/dic_20191203.pickle')\n",
    "ls = list(dic.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 25.4 ms\n"
     ]
    }
   ],
   "source": [
    "def inner_CV(corp, medel1, method, model2_name):\n",
    "    ''' \n",
    "    speficify the parameters to tune\n",
    "    '''\n",
    "    if model2_name =='LR':\n",
    "        spaces = {'model1': model1, 'model2': LogisticRegression, 'method': method,\n",
    "                  'p2': p2_lgbm, 'corp': corp}\n",
    "    elif model2_name =='LGBM':\n",
    "        spaces = {'model1': model1, 'model2': LGBMClassifier, 'method': method,\n",
    "          'p2': p2_lgbm, 'corp': corp}\n",
    "    else:\n",
    "        print('cannot detect this model2')\n",
    "        return\n",
    "    \n",
    "    ''' how fmin2 works\n",
    "    Args:\n",
    "        fn: model to run, in the form of function\n",
    "        space: dict of potential parameters\n",
    "    Return:\n",
    "        best: dict of best parameters\n",
    "    ''' \n",
    "    best_space = fmin2(fn = objective, space=spaces, max_evals = 10)\n",
    "    print(best_space)\n",
    "    model2 = best_space['model2'](**best_space['p2'])\n",
    "    return model2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example of training the best model in our paper\n",
    "word embedding (glove) and nutritional properties: ['wv','nu','scale']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state 2022\n",
      "outer fold\n",
      "2019-12-03 15:48:44.590906\n",
      "{'class_weight': 'balanced', 'boosting': 'gbrt', 'num_leaves': 256, 'max_depth': 16, 'learning_rate': 0.15, 'gamma': 0.65, 'n_estimators': 2000, 'lambda_l2': 1, 'feature_fraction': 0.5, 'bagging_fraction': 0.75, 'bagging_freq': 5, 'subsample': 0.7}\n",
      "2019-12-03 15:48:44.780922\n",
      "[0.8343558282208587, 0.8322981366459627, 0.802547770700637, 0.8674698795180723, 0.8917197452229298]\n",
      "0.8456782720616921\n",
      "-0.8456782720616921 -0.8456782720616921\n",
      "{'class_weight': 'balanced', 'boosting': 'gbrt', 'num_leaves': 512, 'max_depth': 8, 'learning_rate': 0.15, 'gamma': 0.55, 'n_estimators': 2000, 'lambda_l2': 1, 'feature_fraction': 0.75, 'bagging_fraction': 0.5, 'bagging_freq': 5, 'subsample': 0.7}\n",
      "2019-12-03 15:49:59.298873\n",
      "[0.8263473053892216, 0.8383233532934131, 0.7792207792207793, 0.8520710059171598, 0.8875]\n",
      "0.8366924887641147\n",
      "-0.8366924887641147 -0.8456782720616921\n",
      "{'class_weight': 'balanced', 'boosting': 'gbrt', 'num_leaves': 64, 'max_depth': 8, 'learning_rate': 0.1, 'gamma': 0.45, 'n_estimators': 2000, 'lambda_l2': 3, 'feature_fraction': 0.75, 'bagging_fraction': 0.9, 'bagging_freq': 10, 'subsample': 1}\n",
      "2019-12-03 15:51:22.799907\n",
      "[0.8072289156626505, 0.8313253012048193, 0.802547770700637, 0.8727272727272727, 0.8917197452229298]\n",
      "0.841109801103662\n",
      "-0.841109801103662 -0.8456782720616921\n",
      "{'class_weight': 'balanced', 'boosting': 'gbrt', 'num_leaves': 32, 'max_depth': 8, 'learning_rate': 0.15, 'gamma': 0.45, 'n_estimators': 2000, 'lambda_l2': 0.1, 'feature_fraction': 0.75, 'bagging_fraction': 0.75, 'bagging_freq': 5, 'subsample': 1}\n",
      "2019-12-03 15:53:08.557724\n",
      "[0.8313253012048193, 0.8271604938271605, 0.8, 0.874251497005988, 0.8930817610062892]\n",
      "0.8451638106088515\n",
      "-0.8451638106088515 -0.8456782720616921\n",
      "{'class_weight': 'balanced', 'boosting': 'gbrt', 'num_leaves': 64, 'max_depth': 4, 'learning_rate': 0.15, 'gamma': 0.55, 'n_estimators': 2000, 'lambda_l2': 50, 'feature_fraction': 0.75, 'bagging_fraction': 0.9, 'bagging_freq': 5, 'subsample': 0.9}\n",
      "2019-12-03 15:54:15.935932\n",
      "[0.8170731707317075, 0.830188679245283, 0.7866666666666667, 0.8622754491017963, 0.8819875776397514]\n",
      "0.835638308677041\n",
      "-0.835638308677041 -0.8456782720616921\n",
      "{'class_weight': 'balanced', 'boosting': 'gbrt', 'num_leaves': 512, 'max_depth': 16, 'learning_rate': 0.1, 'gamma': 0.45, 'n_estimators': 2000, 'lambda_l2': 50, 'feature_fraction': 0.75, 'bagging_fraction': 0.9, 'bagging_freq': 10, 'subsample': 1}\n",
      "2019-12-03 15:55:39.873109\n",
      "[0.8242424242424242, 0.8220858895705522, 0.802547770700637, 0.8571428571428572, 0.8875]\n",
      "0.838703788331294\n",
      "-0.838703788331294 -0.8456782720616921\n",
      "{'class_weight': 'balanced', 'boosting': 'gbrt', 'num_leaves': 512, 'max_depth': 8, 'learning_rate': 0.15, 'gamma': 0.45, 'n_estimators': 2000, 'lambda_l2': 3, 'feature_fraction': 0.75, 'bagging_fraction': 0.75, 'bagging_freq': 10, 'subsample': 1}\n",
      "2019-12-03 15:57:26.697817\n",
      "[0.8313253012048193, 0.825, 0.7922077922077922, 0.8520710059171598, 0.8930817610062892]\n",
      "0.8387371720672121\n",
      "-0.8387371720672121 -0.8456782720616921\n",
      "{'class_weight': 'balanced', 'boosting': 'gbrt', 'num_leaves': 256, 'max_depth': 16, 'learning_rate': 0.15, 'gamma': 0.45, 'n_estimators': 2000, 'lambda_l2': 10, 'feature_fraction': 0.75, 'bagging_fraction': 0.9, 'bagging_freq': 5, 'subsample': 1}\n",
      "2019-12-03 15:58:44.855929\n",
      "[0.8313253012048193, 0.8322981366459627, 0.7870967741935484, 0.8809523809523809, 0.8805031446540881]\n",
      "0.8424351475301599\n",
      "-0.8424351475301599 -0.8456782720616921\n",
      "{'class_weight': 'balanced', 'boosting': 'gbrt', 'num_leaves': 512, 'max_depth': 4, 'learning_rate': 0.15, 'gamma': 0.55, 'n_estimators': 2000, 'lambda_l2': 0.1, 'feature_fraction': 0.75, 'bagging_fraction': 0.75, 'bagging_freq': 10, 'subsample': 0.7}\n",
      "2019-12-03 16:00:17.580457\n",
      "[0.8121212121212122, 0.8170731707317075, 0.8, 0.8674698795180723, 0.875]\n",
      "0.8343328524741984\n",
      "-0.8343328524741984 -0.8456782720616921\n",
      "{'class_weight': 'balanced', 'boosting': 'gbrt', 'num_leaves': 64, 'max_depth': 4, 'learning_rate': 0.1, 'gamma': 0.45, 'n_estimators': 2000, 'lambda_l2': 1, 'feature_fraction': 0.75, 'bagging_fraction': 0.9, 'bagging_freq': 10, 'subsample': 1}\n",
      "2019-12-03 16:01:41.823511\n",
      "[0.7951807228915663, 0.8433734939759034, 0.810126582278481, 0.8727272727272727, 0.875]\n",
      "0.8392816143746448\n",
      "-0.8392816143746448 -0.8456782720616921\n",
      "{'model1': <models.features.pretrained_wrap object at 0x7f3d7815e048>, 'model2': <class 'lightgbm.sklearn.LGBMClassifier'>, 'method': ['wv', 'nu', 'scale'], 'p2': {'class_weight': 'balanced', 'boosting': 'gbrt', 'num_leaves': 256, 'max_depth': 16, 'learning_rate': 0.15, 'gamma': 0.65, 'n_estimators': 2000, 'lambda_l2': 1, 'feature_fraction': 0.5, 'bagging_fraction': 0.75, 'bagging_freq': 5, 'subsample': 0.7}, 'corp': <models.features.fixed_makedata object at 0x7f3d7815eac8>}\n",
      "0.8516746411483254\n",
      "outer fold\n",
      "2019-12-03 16:03:18.169772\n",
      "{'class_weight': 'balanced', 'boosting': 'gbrt', 'num_leaves': 256, 'max_depth': 16, 'learning_rate': 0.15, 'gamma': 0.65, 'n_estimators': 2000, 'lambda_l2': 1, 'feature_fraction': 0.5, 'bagging_fraction': 0.75, 'bagging_freq': 5, 'subsample': 0.7}\n",
      "2019-12-03 16:03:18.373850\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "0. start the model selection and evaluation\n",
    "'''\n",
    "dfs = []\n",
    "corp0 = fixed_makedata(dic, ls, tag='AMT')\n",
    "knowns = corp0.knowns\n",
    "state = 2022\n",
    "model1 = pretrained_wrap(knowns, 'glove-wiki-gigaword-300')\n",
    "\n",
    "for method in [['wv','nu','scale']]:\n",
    "    corp = copy.deepcopy(corp0)\n",
    "    results = []\n",
    "    model2, fold_count = '', 0\n",
    "    print('state', state)\n",
    "    ss = StratifiedKFold(n_splits = 5, shuffle = True, random_state = state)\n",
    "    for train_index, test_index in ss.split(corp.ls, corp.y):\n",
    "        print('outer fold')\n",
    "        print_time()\n",
    "        '''\n",
    "        1. model selection\n",
    "        ::: model1 is for feature engineering\n",
    "            e.g. model1 = pretrained_wrap(corp.knowns, 'glove-wiki-gigaword-300')\n",
    "            e.g. model1 = gensim_wrap(corp.knowns, Word2Vec, params = {})\n",
    "        ::: model2 is a classifier\n",
    "            e.g. model2 = copy.deepcopy(classifier['default_Logistic'])\n",
    "        '''\n",
    "        model2 = inner_CV(corp.replace(train_index), model1, method, 'LGBM')\n",
    "        '''\n",
    "        2. build X\n",
    "        '''\n",
    "        data = inputs_generater(corp.replace(train_index), model1, method)\n",
    "        '''\n",
    "        3. run and find the best prob threshold\n",
    "        '''\n",
    "        threshold = clf_running_search(data, model1, copy.deepcopy(model2), method)\n",
    "        '''\n",
    "        4. run again and evaluate on the real test set\n",
    "        ''' \n",
    "        data = inputs_generater(corp.add_train_test(train_index, test_index), model1, method)\n",
    "        result = clf_running(data, model1, copy.deepcopy(model2) ,method, threshold).result\n",
    "        result.update({'model1': model1,'tag': corp.tag, 'method':method})\n",
    "        results.append(result)\n",
    "        print(result['test_f1'])\n",
    "\n",
    "    pickle_path = auto_save_pickle(results)\n",
    "    df = pickle2df(results, pickle_path)\n",
    "    dfs.append(df)\n",
    "auto_save_csv(pd.concat(dfs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examples of training these models with logistic regression\n",
    "Naive Bayes BoW: ['wc','nb'] <br>\n",
    "Nutritions only: ['nu','scale'] <br>\n",
    "Nutritions + Naive Bayes BoW: ['wc','nb','nu','scale']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "0. start the model selection and evaluation\n",
    "'''\n",
    "dfs = []\n",
    "corp0 = fixed_makedata(dic, ls, tag='AMT')\n",
    "knowns = corp0.knowns\n",
    "model1 = None\n",
    "state = 2022\n",
    "p2 = p2_lr\n",
    "\n",
    "for method in [['wc','nb'], ['nu','scale'], ['wc','nb','nu','scale']]:\n",
    "    print('state', state)\n",
    "    corp = copy.deepcopy(corp0)\n",
    "    results = []\n",
    "    model2, fold_count = '', 0\n",
    "    ss = StratifiedKFold(n_splits = 5, shuffle = True, random_state = state)\n",
    "    for train_index, test_index in ss.split(corp.ls, corp.y):\n",
    "        print('outer fold')\n",
    "        print_time()\n",
    "        '''\n",
    "        1. model selection\n",
    "        ::: model1 is for feature engineering\n",
    "            e.g. model1 = pretrained_wrap(corp.knowns, 'glove-wiki-gigaword-300')\n",
    "            e.g. model1 = gensim_wrap(corp.knowns, Word2Vec, params = {})\n",
    "        ::: model2 is a classifier\n",
    "            e.g. model2 = copy.deepcopy(classifier['default_Logistic'])\n",
    "        '''\n",
    "        model2 = inner_CV(corp.replace(train_index), model1, method, 'LR')\n",
    "        '''\n",
    "        2. build X\n",
    "        '''\n",
    "        data = inputs_generater(corp.replace(train_index), model1, method)\n",
    "        '''\n",
    "        3. run and find the best prob threshold\n",
    "        '''\n",
    "        threshold = clf_running_search(data, model1, copy.deepcopy(model2), method)\n",
    "        '''\n",
    "        4. run again and evaluate on the real test set\n",
    "        ''' \n",
    "        data = inputs_generater(corp.add_train_test(train_index, test_index), model1, method)\n",
    "        result = clf_running(data, model1, copy.deepcopy(model2), method, threshold).result\n",
    "        result.update({'model1': model1,'tag': corp.tag, 'method':method})\n",
    "        results.append(result)\n",
    "        print(result['test_f1'])\n",
    "\n",
    "    pickle_path = auto_save_pickle(results)\n",
    "    df = pickle2df(results, pickle_path)\n",
    "    dfs.append(df)\n",
    "auto_save_csv(pd.concat(dfs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
